import discord
from discord.ext import commands
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config
from torch.optim import Adam
import random
import os
import traceback

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ—Ç–∞
TOKEN = ''
intents = discord.Intents.default()
intents.messages = True
intents.message_content = True

bot = commands.Bot(command_prefix="!", intents=intents)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
MODEL_SAVE_PATH = "./saved_model"
os.makedirs(MODEL_SAVE_PATH, exist_ok=True)

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏–µ–º
retrain =  False  # –ï—Å–ª–∏ True, –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è; –µ—Å–ª–∏ False, –∑–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
is_train = False # –ï—Å–ª–∏ True, –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ; –µ—Å–ª–∏ False, —Å—Ä–∞–∑—É –Ω–∞—á–∏–Ω–∞–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ T5
model_name = "cointegrated/rut5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
if retrain:
    print("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è...")
    config = T5Config.from_pretrained(model_name)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
    model = T5ForConditionalGeneration(config)  # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è –±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤
else:
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    model = T5ForConditionalGeneration.from_pretrained(MODEL_SAVE_PATH)

# –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ GPU
model.to(device)

# –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
chat_history = []

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
import time
def generate_response(context, max_new_tokens=50):
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    tm1=time.time()
    inputs = tokenizer(context, return_tensors="pt", truncation=True, max_length=512, padding=True)
    
    # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ GPU
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    outputs = model.generate(
        **inputs,  # –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        max_new_tokens=max_new_tokens,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å.
        num_return_sequences=1,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞).
        no_repeat_ngram_size=2,  # –ó–∞–ø—Ä–µ—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ n-–≥—Ä–∞–º–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –±–∏–≥—Ä–∞–º–º). –£–º–µ–Ω—å—à–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è.
        repetition_penalty=1.2,  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤. –ó–Ω–∞—á–µ–Ω–∏–µ >1.0 —É–º–µ–Ω—å—à–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è.
        top_k=30,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–±–æ—Ä —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–æ `k` –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤.
        top_p=0.9,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–±–æ—Ä —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø–æ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (nucleus sampling).
        do_sample=True,  # –í–∫–ª—é—á–∞–µ—Ç sampling (–≤—ã–±–æ—Ä–∫—É), –¥–µ–ª–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π.
        temperature=0.7,  # –†–µ–≥—É–ª–∏—Ä—É–µ—Ç "–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å". –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –º–µ–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–π.
        #length_penalty=0.1,  # –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç —à—Ç—Ä–∞—Ñ –∑–∞ –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. >1.0 –ø–æ–æ—â—Ä—è–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, <1.0 ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ.
        use_cache=True  # –í–∫–ª—é—á–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö.
    )

    
    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(time.time()-tm1)
    return response

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
def train_model_on_chat():
    if len(chat_history) < 2:
        print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")
        return
    print("–¥–ª–∏–Ω–∞",len(chat_history))
    optimizer = Adam(model.parameters(), lr=1e-4)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º Adam
    model.train()

    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã "–∫–æ–Ω—Ç–µ–∫—Å—Ç-–æ—Ç–≤–µ—Ç"
    contexts = []
    targets = []
    for i in range(1, len(chat_history)):
        prev_message = chat_history[i - 1]
        current_message = chat_history[i]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∞–≤—Ç–æ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π —Ä–∞–∑–Ω—ã–µ
        if prev_message["author"] != current_message["author"]:
            targets.append(prev_message["content"])
            contexts.append(current_message["content"])

    if not contexts or not targets:
        print("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")
        return

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö

    inputs = tokenizer(contexts, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
    labels = tokenizer(targets, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
    
    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª–∏–Ω input_ids –∏ labels
    min_length = min(inputs["input_ids"].shape[1], labels["input_ids"].shape[1])
    inputs["input_ids"] = inputs["input_ids"][:, :min_length]
    inputs["attention_mask"] = inputs["attention_mask"][:, :min_length]
    labels["input_ids"] = labels["input_ids"][:, :min_length]

    # –ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –Ω–∞ GPU
    inputs = {k: v.to(device) for k, v in inputs.items()}
    labels = labels["input_ids"].to(device)

    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –±–∞—Ç—á–∏
    batch_size = 4
    total_batches = (len(inputs["input_ids"]) + batch_size - 1) // batch_size  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π

    # –î–æ–±–∞–≤–ª—è–µ–º 5 —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
    epochs = 200
    for epoch in range(epochs):
        print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}")
        for batch_idx in range(total_batches):
            
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(inputs["input_ids"]))

            batch_inputs = {
                "input_ids": inputs["input_ids"][start_idx:end_idx],
                "attention_mask": inputs["attention_mask"][start_idx:end_idx]
            }
            batch_labels = labels[start_idx:end_idx]

            # –ü—Ä–æ—Å—Ç–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
            optimizer.zero_grad()
            outputs = model(input_ids=batch_inputs["input_ids"], attention_mask=batch_inputs["attention_mask"], labels=batch_labels)
            loss = outputs.loss

            # –î–æ–±–∞–≤–ª—è–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            penalty_loss = add_penalty_for_repetition(batch_inputs["input_ids"], batch_labels)
            loss += penalty_loss

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–≥—Ä–∞–¥—É –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            #reward = add_reward_for_word_count(batch_inputs["input_ids"], batch_labels)
            #loss -= reward

            loss.backward()
            optimizer.step()

            print(f"–û–±—É—á–µ–Ω–∏–µ –±–∞—Ç—á–∞ {batch_idx + 1}/{total_batches}, Loss: {loss.item()}")
        save_model()
    model.eval()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—Ç—Ä–∞—Ñ–∞ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
def add_penalty_for_repetition(input_ids, labels):
    penalty = 0.0
    batch_size, seq_len = labels.shape
    for i in range(batch_size):
        label_text = tokenizer.decode(labels[i], skip_special_tokens=True)
        words = label_text.split()  # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞
        
        # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç—ã –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
        word_counts = {}
        for word in words:
            if word not in word_counts:
                word_counts[word] = 0
            word_counts[word] += 1
        
        # –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        for count in word_counts.values():
            if count > 1:  # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è
                penalty += (count - 1) * 0.2  # –®—Ç—Ä–∞—Ñ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π

    return torch.tensor(penalty, device=device)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
def add_reward_for_word_count(input_ids, labels):
    reward = 0.0
    batch_size, seq_len = labels.shape
    for i in range(batch_size):
        input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)
        label_text = tokenizer.decode(labels[i], skip_special_tokens=True)
        
        # –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –æ—Ç–≤–µ—Ç–µ
        input_word_count = len(input_text.split())
        label_word_count = len(label_text.split())
        
        # –ù–∞–≥—Ä–∞–¥–∞ –¥–∞—ë—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–Ω–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        #if label_word_count > input_word_count:
        #    reward += (label_word_count - input_word_count) * 0.1  # –ù–µ–±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –∫–∞–∂–¥–æ–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
    return torch.tensor(reward, device=device)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
def save_model():
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model.save_pretrained(MODEL_SAVE_PATH)
    tokenizer.save_pretrained(MODEL_SAVE_PATH)
    print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 1000 —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∫–∞–Ω–∞–ª–∞
async def fetch_initial_messages(channel):
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 1000 —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    async for message in channel.history(limit=500):
        chat_history.append({
            "author": message.author.name,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º—è –∞–≤—Ç–æ—Ä–∞
            "content": message.content     # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
        })
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chat_history)} —Å–æ–æ–±—â–µ–Ω–∏–π.")
    if is_train:
        try:
            train_model_on_chat()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö –∏–∑ –∫–∞–Ω–∞–ª–∞ {channel.name}: {e}")
            traceback.print_exc()
# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
@bot.event
async def on_ready():
    print(f"–ë–æ—Ç {bot.user} –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ Discord!")
    for guild in bot.guilds:
        for channel in guild.text_channels:
            if channel.name == "üåê-o–±—â–∏–π-—á–∞—Ç":
                try:
                    await fetch_initial_messages(channel)
                    if not is_train:
                        break
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∫–∞–Ω–∞–ª–∞ {channel.name}: {e}")

@bot.event
async def on_message(message):
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç —Å–∞–º–æ–≥–æ –±–æ—Ç–∞
    if message.author == bot.user:
        return

    # –°–ø–∏—Å–æ–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    allowed_channel_ids = {
        1355493651974717540,  # –í—Ç–æ—Ä–æ–π –∫–∞–Ω–∞–ª
        1355606855144968202   # –ß–µ—Ç–≤—ë—Ä—Ç—ã–π –∫–∞–Ω–∞–ª
    }

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω–æ–º –∫–∞–Ω–∞–ª–µ
        if message.channel.id not in allowed_channel_ids:
            return  # –ï—Å–ª–∏ –∫–∞–Ω–∞–ª –Ω–µ —Ä–∞–∑—Ä–µ—à—ë–Ω, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∞–≤—Ç–æ—Ä–∞
        chat_history.append({
            "author": message.author.name,
            "content": message.content
        })
        if len(chat_history) > 1000:
            chat_history.pop(0)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        context = "\n".join([msg["content"] for msg in chat_history[-1:]])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        response = generate_response(context, max_new_tokens=50)
        print(context)
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ —Ç–æ—Ç –∂–µ –∫–∞–Ω–∞–ª
        await message.channel.send(response)

    except Exception as e:
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        traceback.print_exc()

# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
bot.run(TOKEN)
